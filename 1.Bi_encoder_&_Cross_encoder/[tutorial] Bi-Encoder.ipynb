{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Bert ë…¼ë¬¸ ìš”ì•½\n",
    "\n",
    "- Bertë¡œ Sentence Embeddingì„ êµ¬í˜„í•˜ëŠ” ë°©ë²• ì†Œê°œ\n",
    "\n",
    "- ë°ì´í„° ìœ í˜•(Classification, Regression)ì— ë§ëŠ” Fine-tuning êµ¬ì¡° ì†Œê°œ\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Network(ìƒ´ ë„¤íŠ¸ì›Œí¬)\n",
    "\n",
    "- ìƒ´ ë„¤íŠ¸ì›Œí¬(siam network)ëŠ” í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‘ ê°œì˜ outputì„ ì‚°ì¶œí•˜ëŠ” êµ¬ì¡°ë¥¼ ë§í•¨.\n",
    "\n",
    "- ëª¨ë¸ì„ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ì§€ë§Œ ê°œë³„ ë°ì´í„°ê°€ ê°ê° ì…ë ¥ë˜ê¸° ë•Œë¬¸ì— ë°ì´í„° ê°„ ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ëŠ” êµ¬ì¡°ì„.\n",
    "\n",
    "- Bi eoncoderëŠ” ìƒ´ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì´ë¯€ë¡œ ë¬¸ì¥ ê°„ ì—°ê´€ì„± íŒŒì•…ì„ ìœ„í•´ì„  cosine ìœ ì‚¬ë„ê°€ í•„ìš”í•¨.\n",
    "\n",
    "- ë°˜ë©´ cross encoderëŠ” ë‘ ê°œì˜ ë¬¸ì¥ì„ í•˜ë‚˜ì˜ input ë°ì´í„°ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, Model ë‚´ë¶€ì— ìˆëŠ” Attentionìœ¼ë¡œ ë¬¸ì¥ ê°„ ì—°ê´€ì„±ì„ íŒŒì•…í•¨.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Modelë¡œ Bi encoder êµ¬í˜„í•˜ê¸°\n",
    "\n",
    "- Pre-trained Modelì˜ Encoder ìµœì¢… ì¶œë ¥ ê²°ê³¼(last_hidden_state)ë¥¼ í™œìš©í•´ Bi-encoder êµ¬í˜„í•¨.\n",
    "\n",
    "- Pooling ë°©ë²•ì—ëŠ” [CLS] pooling, mean pooling, max poolingì´ í™œìš© ê°€ëŠ¥í•˜ë‚˜ mean poolingì´ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•„ ê¸°ë³¸ ê°’ìœ¼ë¡œ í™œìš©ë¨.\n",
    "\n",
    "- ì•„ë˜ ê·¸ë¦¼ì€ Sentence Bertê°€ Sentence Embeddingì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë‚˜íƒ€ëƒ„.\n",
    "\n",
    "- Bert ëª¨ë¸ì„ ê±°ì¹œ ë¬¸ì¥ì€ Token ê°œìˆ˜ ë§Œí¼ì˜ Embeddingìœ¼ë¡œ í‘œí˜„ë¨. \n",
    "\n",
    "- ì—¬ëŸ¬ ê°œì˜ Embeddingì„ í•˜ë‚˜ì˜ Embeddingìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ Poolingì„ ìˆ˜í–‰í•˜ë©´ Sentence Embeddingì„ ë§Œë“¤ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "    <img src ='../img/SBERT_Architecture.png' alt='SBERT_Architecture' width ='150px'/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentece Bert êµ¬ì¡° ìƒì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ê²°ê³¼ \n",
      " ['ë‚˜', '##ëŠ”', 'ì–´ì œ', 'ë§¥', '##ë¶', '##ì„', 'ìƒ€', '##ë‹¤', '.']\n",
      "\n",
      "PLM output shape => torch.Size([1, 11, 768]) \n",
      "Sbert output shape => torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer, TrainingArguments, TrainerCallback, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "class SentenceBert(nn.Module):\n",
    "    def __init__(self, model, pooling_type=\"mean\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model  # Language model ex)BertModel, ElectraModel ...\n",
    "        self.pooling_type = pooling_type  # pooling type ì €ì¥\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "\n",
    "        # Padding ì œê±°ë¥¼ ìœ„í•´ Attention Mask í™œìš©\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "\n",
    "        # Last_hidden_state ì¶”ì¶œ\n",
    "        features = self.model(**kwargs)\n",
    "        last_hidden_state = features[\"last_hidden_state\"]\n",
    "\n",
    "        if self.pooling_type == \"cls\":\n",
    "            \"\"\"\n",
    "            [cls] ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "            \"\"\"\n",
    "\n",
    "            cls_token = last_hidden_state[:, 0]  # [batch_size, embed_size]\n",
    "            result = cls_token\n",
    "\n",
    "        if self.pooling_type == \"max\":\n",
    "            \"\"\"\n",
    "            ë¬¸ì¥ ë‚´ í† í° ì¤‘ ê°€ì¥ ê°’ì´ í° tokenë§Œ ì¶”ì¶œ\n",
    "            \"\"\"\n",
    "\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # Set padding tokens to large negative value\n",
    "            last_hidden_state[input_mask_expanded == 0] = -1e9\n",
    "            max_over_time = torch.max(last_hidden_state, 1)[0]\n",
    "            result = max_over_time\n",
    "\n",
    "        if self.pooling_type == \"mean\":\n",
    "            \"\"\"\n",
    "            ë¬¸ì¥ ë‚´ í† í°ì„ í•©í•œ ë’¤ í‰ê· \n",
    "            \"\"\"\n",
    "            # padding ë¶€ë¶„ ì°¾ê¸° = [batch_size, src_token, embed_size]\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            )\n",
    "            # paddingì¸ ê²½ìš° 0 ì•„ë‹Œ ê²½ìš° 1ê³±í•œ ë’¤ ì´í•© = [batch_size, embed_size]\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "\n",
    "            # í‰ê·  ë‚´ê¸°ìœ„í•œ token ê°œìˆ˜\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "            result = sum_embeddings / sum_mask\n",
    "\n",
    "        #  input shape   : [batch_size, src_token, embed_size]\n",
    "        #                               â¬‡ï¸\n",
    "        #  output shape : [batch_size, embed_size]\n",
    "        return {\"sentence_embedding\": result}\n",
    "\n",
    "\n",
    "### Sbert ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "\n",
    "sbert = SentenceBert(model)\n",
    "\n",
    "sen = \"ë‚˜ëŠ” ì–´ì œ ë§¥ë¶ì„ ìƒ€ë‹¤.\"\n",
    "\n",
    "token = tokenizer(sen, return_tensors=\"pt\")\n",
    "PLM = model(**token)[\"last_hidden_state\"]\n",
    "sentence_embedding = sbert(**token)[\"sentence_embedding\"]\n",
    "\n",
    "print(\"Tokenizing ê²°ê³¼ \\n\", tokenizer.tokenize(sen))\n",
    "print(\"\")\n",
    "print(f\"PLM output shape => {PLM.shape} \\nSbert output shape => {sentence_embedding.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ìœ í˜• ë³„ Sentence Bert í•™ìŠµ êµ¬ì¡°\n",
    "\n",
    "- SentenceBertë¥¼ í•™ìŠµ ì‹œí‚¤ëŠ” ë°©ë²•ì€ í•™ìŠµ ë°ì´í„°ì˜ ìœ í˜•ì— ë”°ë¼ ë‹¬ë¼ì§\n",
    "\n",
    "- Regression ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Sentence Bertë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²½ìš° í•™ìŠµ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ.\n",
    "\n",
    "    <img src ='../img/SBERT_Siamese_Network.png' alt='SBERT_Siamese_Network' width ='300px'/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "- Classification ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Sentence Bertë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²½ìš° í•™ìŠµ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ.\n",
    "\n",
    "    <img src ='../img/SBERT_SoftmaxLoss.png' alt='SBERT_SoftmaxLoss' width ='300px'/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regeression ë°ì´í„° ê¸°ë°˜ì˜ í•™ìŠµ êµ¬ì¡°\n",
    "\n",
    "- ë…¼ë¬¸ì—ì„œëŠ” Regression ë°ì´í„°ë¡œ STS ë°ì´í„°ë¥¼ í™œìš©í•¨. ì´ ê¸€ì—ì„œëŠ” `KorSTS` ë°ì´í„°ë¥¼ í™œìš©í•¨.\n",
    "\n",
    "- STS ë°ì´í„°ëŠ” ë¬¸ì¥ 2ê°œì™€ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ í‘œí˜„í•œ ê°’ìœ¼ë¡œ êµ¬ì„±ë¨.\n",
    "\n",
    "  ```python\n",
    "\n",
    "  {\n",
    "  'sen1': 'ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.',\n",
    "  'sen2': 'ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.',\n",
    "  'score': '5.000'\n",
    "  }\n",
    "\n",
    "  ```\n",
    "\n",
    "- í•™ìŠµì´ ì™„ë£Œëœ ì´í›„ì—ëŠ” í•™ìŠµ êµ¬ì¡°ì—ì„œ Sbertë¥¼ ì¶”ì¶œí•˜ì—¬ í™œìš©í•¨.\n",
    "\n",
    "    <img src='../img/SBERT_Siamese_Network.png' alt='siamese' width='300px'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelForRegressionTraining(nn.Module):\n",
    "    def __init__(self, model, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # í•™ìŠµì„ ìˆ˜í–‰í•  ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, features, answer):\n",
    "\n",
    "        # Sentence 1, Sentence 2ì— ëŒ€í•œ Sentence Embedding í™•ë³´\n",
    "        embeddings = [self.model(**input_data)[\"sentence_embedding\"] for input_data in features]\n",
    "\n",
    "        u, v = embeddings[0], embeddings[1]\n",
    "\n",
    "        # Sentence 1, Sentence 2ì— ëŒ€í•œ Cosine Similarity ê³„ì‚°\n",
    "        cos_score_transformation = nn.Identity()\n",
    "        outputs = cos_score_transformation(torch.cosine_similarity(u, v))\n",
    "\n",
    "        # label score Normalization\n",
    "        answer = answer / 5  # 0 ~ 5 => 0 ~ 1\n",
    "\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(outputs, answer.view(-1))\n",
    "\n",
    "        return {\"loss\": loss}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KorSTS Data ë¶ˆëŸ¬ì˜¤ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>í•œ ë‚¨ìê°€ í° í”Œë£¨íŠ¸ë¥¼ ì—°ì£¼í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>ë‚¨ìê°€ í”Œë£¨íŠ¸ë¥¼ ì—°ì£¼í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>í•œ ë‚¨ìê°€ í”¼ìì— ì¹˜ì¦ˆë¥¼ ë¿Œë ¤ë†“ê³  ìˆë‹¤.</td>\n",
       "      <td>í•œ ë‚¨ìê°€ êµ¬ìš´ í”¼ìì— ì¹˜ì¦ˆ ì¡°ê°ì„ ë¿Œë ¤ë†“ê³  ìˆë‹¤.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sen1                          sen2  score\n",
       "0           ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.                 ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.  5.000\n",
       "1   í•œ ë‚¨ìê°€ í° í”Œë£¨íŠ¸ë¥¼ ì—°ì£¼í•˜ê³  ìˆë‹¤.             ë‚¨ìê°€ í”Œë£¨íŠ¸ë¥¼ ì—°ì£¼í•˜ê³  ìˆë‹¤.  3.800\n",
       "2  í•œ ë‚¨ìê°€ í”¼ìì— ì¹˜ì¦ˆë¥¼ ë¿Œë ¤ë†“ê³  ìˆë‹¤.  í•œ ë‚¨ìê°€ êµ¬ìš´ í”¼ìì— ì¹˜ì¦ˆ ì¡°ê°ì„ ë¿Œë ¤ë†“ê³  ìˆë‹¤.  3.800"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/KorSTS/sts-train.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data = data[[\"sentence1\", \"sentence2\", \"score\"]]\n",
    "data.columns = [\"sen1\", \"sen2\", \"score\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Datasetsìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "- ğŸ¤—Transformersì™€ í˜¸í™˜ì„ ìœ„í•´ Dataframeì„ ğŸ¤—datasetìœ¼ë¡œ ë³€í™˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': 'ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.', 'sen2': 'ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.', 'score': '5.000'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator êµ¬í˜„\n",
    "\n",
    "- í•™ìŠµ êµ¬ì¡°ì— ë§ëŠ” input data ìƒì„±ì„ ìœ„í•œ custom collator ì œì‘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"score\":\n",
    "                labels.append(float(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        token = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(token)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤—Transformers Trainerë¥¼ í™œìš©í•´ í•™ìŠµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Option ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# í•™ìŠµ êµ¬ì¡° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_for_training = modelForRegressionTraining(sbert)\n",
    "\n",
    "# Trainer ì •ì˜\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regeression ë°ì´í„° ê¸°ë°˜ì˜ í•™ìŠµ êµ¬ì¡°\n",
    "\n",
    "- ë…¼ë¬¸ì—ì„œëŠ” Regression ë°ì´í„°ë¡œ NLI ë°ì´í„°ë¥¼ í™œìš©í•¨. ì´ ê¸€ì—ì„œëŠ” `KorNLI` ë°ì´í„° ì¤‘ `snli_1.0_train.ko`ë¥¼ í™œìš©í•¨.\n",
    "\n",
    "- KorNLI ë°ì´í„°ëŠ” ë¬¸ì¥ 2ê°œì™€ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ Labelë¡œ í‘œí˜„í•¨.\n",
    "\n",
    "```python\n",
    "\n",
    "  {\n",
    "   'sen1': 'ê·¸ë¦¬ê³  ê·¸ê°€ ë§í–ˆë‹¤, \"ì—„ë§ˆ, ì € ì™”ì–´ìš”.\"',\n",
    "   'sen2': 'ê·¸ëŠ” í•™êµ ë²„ìŠ¤ê°€ ê·¸ë¥¼ ë‚´ë ¤ì£¼ìë§ˆì ì—„ë§ˆì—ê²Œ ì „í™”ë¥¼ ê±¸ì—ˆë‹¤.',\n",
    "   'gold_label': 'neutral'\n",
    "   }\n",
    "\n",
    "```\n",
    "\n",
    "- í•™ìŠµì´ ì™„ë£Œëœ ì´í›„ì—ëŠ” í•™ìŠµ êµ¬ì¡°ì—ì„œ Sbertë¥¼ ì¶”ì¶œí•˜ì—¬ í™œìš©í•¨.\n",
    "\n",
    "    <img src='../img/SBERT_SoftmaxLoss.png' alt='siamese' width='300px'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class modelForClassificationTraining(nn.Module):\n",
    "    def __init__(self, model, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # í•™ìŠµí•  ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        self.model = model\n",
    "\n",
    "        # ëª¨ë¸ embed_size\n",
    "        sentence_embedding_dimension = self.model.model.config.hidden_size\n",
    "\n",
    "        # concat í•´ì•¼í•˜ëŠ” vector ê°œìˆ˜(U,V, |U-V|)\n",
    "        num_vectors_concatenated = 3\n",
    "\n",
    "        # embed_size * 3 => 3 ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œì‹œí‚¤ëŠ” classifier\n",
    "        self.classifier = nn.Linear(num_vectors_concatenated * sentence_embedding_dimension, 3)\n",
    "\n",
    "    def forward(self, features, answer):\n",
    "\n",
    "        # Sentence Embedding ìƒì„±\n",
    "        embeddings = [self.model(**input_data)[\"sentence_embedding\"] for input_data in features]\n",
    "\n",
    "        u, v = embeddings\n",
    "\n",
    "        # U,V, |U-V| vector ë³‘í•©\n",
    "        vectors_concat = []\n",
    "        vectors_concat.append(u)\n",
    "        vectors_concat.append(v)\n",
    "        vectors_concat.append(torch.abs(u - v))\n",
    "        features = torch.cat(vectors_concat, 1)\n",
    "\n",
    "        # ë³‘í•©í•œ vector ì°¨ì› ì¶•ì†Œ\n",
    "        outputs = self.classifier(features)\n",
    "\n",
    "        # Loss ê³„ì‚°\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, answer.view(-1))\n",
    "\n",
    "        return {\"loss\": loss}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KorNLI Data ë¶ˆëŸ¬ì˜¤ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.</td>\n",
       "      <td>í•œ ì‚¬ëŒì´ ê²½ìŸì„ ìœ„í•´ ë§ì„ í›ˆë ¨ì‹œí‚¤ê³  ìˆë‹¤.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.</td>\n",
       "      <td>í•œ ì‚¬ëŒì´ ì‹ë‹¹ì—ì„œ ì˜¤ë¯ˆë ›ì„ ì£¼ë¬¸í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.</td>\n",
       "      <td>ì‚¬ëŒì€ ì•¼ì™¸ì—ì„œ ë§ì„ íƒ€ê³  ìˆë‹¤.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sen1                       sen2     gold_label\n",
       "0  ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.  í•œ ì‚¬ëŒì´ ê²½ìŸì„ ìœ„í•´ ë§ì„ í›ˆë ¨ì‹œí‚¤ê³  ìˆë‹¤.        neutral\n",
       "1  ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.   í•œ ì‚¬ëŒì´ ì‹ë‹¹ì—ì„œ ì˜¤ë¯ˆë ›ì„ ì£¼ë¬¸í•˜ê³  ìˆë‹¤.  contradiction\n",
       "2  ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.         ì‚¬ëŒì€ ì•¼ì™¸ì—ì„œ ë§ì„ íƒ€ê³  ìˆë‹¤.     entailment"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"../data/KorNLI/snli_1.0_train.ko.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data.columns = [\"sen1\", \"sen2\", \"gold_label\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gold_label Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.</td>\n",
       "      <td>í•œ ì‚¬ëŒì´ ê²½ìŸì„ ìœ„í•´ ë§ì„ í›ˆë ¨ì‹œí‚¤ê³  ìˆë‹¤.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.</td>\n",
       "      <td>í•œ ì‚¬ëŒì´ ì‹ë‹¹ì—ì„œ ì˜¤ë¯ˆë ›ì„ ì£¼ë¬¸í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.</td>\n",
       "      <td>ì‚¬ëŒì€ ì•¼ì™¸ì—ì„œ ë§ì„ íƒ€ê³  ìˆë‹¤.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sen1                       sen2  gold_label\n",
       "0  ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.  í•œ ì‚¬ëŒì´ ê²½ìŸì„ ìœ„í•´ ë§ì„ í›ˆë ¨ì‹œí‚¤ê³  ìˆë‹¤.           2\n",
       "1  ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.   í•œ ì‚¬ëŒì´ ì‹ë‹¹ì—ì„œ ì˜¤ë¯ˆë ›ì„ ì£¼ë¬¸í•˜ê³  ìˆë‹¤.           0\n",
       "2  ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.         ì‚¬ëŒì€ ì•¼ì™¸ì—ì„œ ë§ì„ íƒ€ê³  ìˆë‹¤.           1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "data[\"gold_label\"] = data[\"gold_label\"].replace(label2int).values\n",
    "\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Datasetìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': 'ë§ì„ íƒ„ ì‚¬ëŒì´ ê³ ì¥ë‚œ ë¹„í–‰ê¸° ìœ„ë¡œ ë›°ì–´ì˜¤ë¥¸ë‹¤.',\n",
       " 'sen2': 'í•œ ì‚¬ëŒì´ ê²½ìŸì„ ìœ„í•´ ë§ì„ í›ˆë ¨ì‹œí‚¤ê³  ìˆë‹¤.',\n",
       " 'gold_label': 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator êµ¬í˜„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"gold_label\":\n",
    "                labels.append(int(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        tokenized = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(tokenized)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤—Transformers Trainerë¥¼ í™œìš©í•´ í•™ìŠµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Option ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# í•™ìŠµ êµ¬ì¡° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_for_training = modelForClassificationTraining(sbert)\n",
    "\n",
    "# Trainer ì •ì˜\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Jun 13 2022, 17:35:03) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
