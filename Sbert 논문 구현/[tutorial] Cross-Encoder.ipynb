{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross encoder êµ¬ì¡°\n",
    "\n",
    "- ì´ ê¸€ì€ Cross Encoderì˜ êµ¬ì¡°ë¥¼ ì†Œê°œí•˜ê³  í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•¨. \n",
    "\n",
    "- Cross encoderëŠ” Pretrained Modelì— Classification layerë¥¼ ìŒ“ì€ êµ¬ì¡°ì„. \n",
    "\n",
    "- Cross EncoderëŠ” ğŸ¤—Transforemersì˜ Sequenceclassification modelì„ ë¶ˆëŸ¬ì™€ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "- Sentence_transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ êµ¬í˜„ëœ Cross_Encoder ë˜í•œ ë‚´ë¶€ì— SequenceClassificationë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ë¨.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "* Cross EncoderëŠ” Pretrained Modelì— Classification Headë¥¼ ì—°ê²°í•œ ëª¨ë¸ì„.\n",
    "\n",
    "    <img src='../images/cross_encoder.png' alt='cross_encoder' width='500px'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ClassificationHead\n",
    "\n",
    "- Classification ìƒì„¸ êµ¬ì¡°ëŠ” dense_layer => gelu => output_pojection_layerë¡œ ë˜ì–´ìˆìŒ.\n",
    "\n",
    "    <img src='../images/classification_head.png' alt='classification_head' width='500px'>\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "- Pre-trained Modelì˜ outputì¸ last_hidden_state ì¤‘ [CLS] í† í°ë§Œì„ Classification Headì˜ input dataë¡œ í™œìš©\n",
    "\n",
    "- ìˆ˜ë§ì€ í† í° embeddingì´ ìˆì§€ë§Œ ê·¸ ì¤‘ [CLS] í† í°ì´ ë‘ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ ìš”ì•½í•œ embeddingì´ë¼ íŒë‹¨í•˜ë¯€ë¡œ ì´ë¥¼ í™œìš©í•¨.  \n",
    "\n",
    "- ì´ì²˜ëŸ¼ ì—¬ëŸ¬ ì •ë³´ë¥¼ í•˜ë‚˜ë¡œ ì¹˜í™˜í•˜ëŠ” ë°©ë²•ì„ poolingì´ë¼ í•¨. pooling ê´€ë ¨í•´ì„œëŠ” [Why it called pooler?](https://github.com/google-research/bert/issues/1102)ë¥¼ ì°¸ê³ .\n",
    "\n",
    "- pooling ëœ embeddingì€ ReLU(ë˜ëŠ” Tanh) í•¨ìˆ˜ë¥¼ ê±°ì¹œ ë‹¤ìŒ projection layerë¥¼ í†µí•´ ë¼ë²¨ í¬ê¸°ì— ë§ëŠ” ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•¨.\n",
    "\n",
    "- Regression ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” ê²½ìš° ë¼ë²¨ ê°œìˆ˜(N)ì€ 1ë¡œ ì„¤ì •í•´ì•¼í•˜ë©°, Classification ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” ê²½ìš° ë¼ë²¨ ê°œìˆ˜ì— ë§ê²Œ Nì„ ì„¤ì •í•´ì•¼í•¨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizerFast, TrainingArguments, Trainer\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from datasets import Dataset\n",
    "from torch import Tensor, nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class classificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout\n",
    "            if config.classifier_dropout is not None\n",
    "            else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.gelu = nn.functional.gelu\n",
    "\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "\n",
    "        # [batch, embed_size] => [batch, num_labels]\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # [CLS] í† í° ì¶”ì¶œ\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # label ê°œìˆ˜ë§Œí¼ ì°¨ì› ì¶•ì†Œ [batch, embed_size] => [batch, num_labels]\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEncoder êµ¬ì¡°\n",
    "\n",
    "- ì•„ë˜ì˜ CrossEncoder êµ¬ì¡°ëŠ” ğŸ¤—Transforemersì˜ Sequenceclassification ë‚´ë¶€ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í–ˆìœ¼ë©°, ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì½”ë“œ ì¼ë¶€ë¥¼ ë³€ê²½í•˜ì˜€ìŒ.\n",
    "\n",
    "- Cross_Encoderì˜ Outputì€ ëª¨ë¸ í•™ìŠµ ì‹œ Lossì™€ Logitsì„ ë°˜í™˜í•˜ê³  í‰ê°€ ë° í™œìš© ì‹œì—ëŠ” Logitsë§Œ ë°˜í™˜í•¨.\n",
    "\n",
    "- í•™ìŠµì— ë°ì´í„°ì— ë”°ë¼ Loss_functionì´ ë‹¬ë¼ì§. í•™ìŠµ ìœ í˜•ì´ Regressionì¼ ë•Œ MSE, Classficationì¼ ë•Œ Cross-Enctropyë¥¼ í™œìš©í•¨.\n",
    "\n",
    "- í•™ìŠµ ìœ í˜•ì— ë”°ë¼ Loss Functionì´ ë‹¬ë¼ì§€ëŠ” ì´ìœ ê°€ ê¶ê¸ˆí•œ ê²½ìš° ë‹¤ìŒì„ ì°¸ê³ \n",
    "    - [In which cases is the cross-entropy preferred over the mean squared error?](https://stackoverflow.com/questions/36515202/in-which-cases-is-the-cross-entropy-preferred-over-the-mean-squared-error)\n",
    "    \n",
    "    - [What is the different between MSE error and Cross-entropy error in NN](https://susanqq.github.io/tmp_post/2017-09-05-crossentropyvsmes/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, model, num_labels) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.config.num_labels = num_labels\n",
    "        self.classifier = classificationHead(self.model.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        discriminator_hidden_states = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Last-hidden-state ì¶”ì¶œ\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "\n",
    "        # Last-hidden-stateë¥¼ classificationHeadì˜ ì…ë ¥ ë°ì´í„°ë¡œ í™œìš©\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.model.config.num_labels == 1:\n",
    "                # Regression Modelì€ MSE Loss í™œìš©\n",
    "                loss_fct = MSELoss()\n",
    "            else:\n",
    "                # classification Modelì€ Cross entropy í™œìš©\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, 3), labels.view(-1))\n",
    "            return {\"loss\": loss, \"logit\": logits}\n",
    "        else:\n",
    "            return {\"logit\": logits}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Encoder í•™ìŠµí•˜ê¸°\n",
    "* ì´ ê¸€ì—ì„  Numerical Dataë¥¼ í™œìš©í•´ Cross Encoderë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ë§Œì„ ì†Œê°œí•¨."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KorSTS Data ë¶ˆëŸ¬ì˜¤ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>í•œ ë‚¨ìê°€ í° í”Œë£¨íŠ¸ë¥¼ ì—°ì£¼í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>ë‚¨ìê°€ í”Œë£¨íŠ¸ë¥¼ ì—°ì£¼í•˜ê³  ìˆë‹¤.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>í•œ ë‚¨ìê°€ í”¼ìì— ì¹˜ì¦ˆë¥¼ ë¿Œë ¤ë†“ê³  ìˆë‹¤.</td>\n",
       "      <td>í•œ ë‚¨ìê°€ êµ¬ìš´ í”¼ìì— ì¹˜ì¦ˆ ì¡°ê°ì„ ë¿Œë ¤ë†“ê³  ìˆë‹¤.</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sen1                          sen2  score\n",
       "0           ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.                 ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.  5.000\n",
       "1   í•œ ë‚¨ìê°€ í° í”Œë£¨íŠ¸ë¥¼ ì—°ì£¼í•˜ê³  ìˆë‹¤.             ë‚¨ìê°€ í”Œë£¨íŠ¸ë¥¼ ì—°ì£¼í•˜ê³  ìˆë‹¤.  3.800\n",
       "2  í•œ ë‚¨ìê°€ í”¼ìì— ì¹˜ì¦ˆë¥¼ ë¿Œë ¤ë†“ê³  ìˆë‹¤.  í•œ ë‚¨ìê°€ êµ¬ìš´ í”¼ìì— ì¹˜ì¦ˆ ì¡°ê°ì„ ë¿Œë ¤ë†“ê³  ìˆë‹¤.  3.800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"../data/KorSTS/sts-train.tsv\") as f:\n",
    "    v = f.readlines()\n",
    "\n",
    "## from list to dataframe\n",
    "lst = [i.rstrip(\"\\n\").split(\"\\t\") for i in v]\n",
    "\n",
    "data = pd.DataFrame(lst[1:], columns=lst[:1])\n",
    "data = data[[\"sentence1\", \"sentence2\", \"score\"]]\n",
    "data.columns = [\"sen1\", \"sen2\", \"score\"]\n",
    "data.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Datasetsìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "- ğŸ¤—Transformersì™€ í˜¸í™˜ì„ ìœ„í•´ Dataframeì„ ğŸ¤—datasetìœ¼ë¡œ ë³€í™˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sen1': 'ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.', 'sen2': 'ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.', 'score': '5.000'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_set = Dataset.from_pandas(data)\n",
    "\n",
    "train_data_set[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collator êµ¬í˜„\n",
    "\n",
    "- í•™ìŠµ êµ¬ì¡°ì— ë§ëŠ” input data ìƒì„±ì„ ìœ„í•œ custom collator ì œì‘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(batch):\n",
    "    text_lst1 = []\n",
    "    text_lst2 = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        for k, v in example.items():\n",
    "            if k == \"sen1\":\n",
    "                text_lst1.append(v)\n",
    "            if k == \"sen2\":\n",
    "                text_lst2.append(v)\n",
    "            if k == \"score\":\n",
    "                labels.append(float(v))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    sentence_features = []\n",
    "    for items in [text_lst1, text_lst2]:\n",
    "        token = tokenizer(items, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        sentence_features.append(token)\n",
    "\n",
    "    return dict(features=sentence_features, answer=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤—Transformers Trainerë¥¼ í™œìš©í•´ í•™ìŠµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Option ì„¤ì •\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps=10,\n",
    "    eval_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "cross_encoder = CrossEncoder(model, num_labels=1) \n",
    "\n",
    "# Huggingfaceì˜ ElectraForSequenceClassificationì„ cross_encoderë¡œ í™œìš©ê°€ëŠ¥\n",
    "# from transformers import ElectraForSequenceClassification\n",
    "# cross_encoder = ElectraForSequenceClassification.from_pretrained('model/disc_book_final',num_labels=3)\n",
    "\n",
    "# Trainer ì •ì˜\n",
    "trainer = Trainer(\n",
    "    model=cross_encoder,\n",
    "    train_dataset=train_data_set,\n",
    "    args=training_args,\n",
    "    data_collator=smart_batching_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
